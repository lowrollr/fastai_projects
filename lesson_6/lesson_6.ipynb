{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n    \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-18T23:25:22.507936Z","iopub.execute_input":"2023-01-18T23:25:22.508432Z","iopub.status.idle":"2023-01-18T23:25:22.529851Z","shell.execute_reply.started":"2023-01-18T23:25:22.508395Z","shell.execute_reply":"2023-01-18T23:25:22.528425Z"},"trusted":true},"execution_count":355,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Lesson 6: Random Forests\n\nIn this mini-project we'll be building a random forest classifier trained on the *Titanic* dataset and attempting to tune it.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nnp.random.seed(99)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.532191Z","iopub.execute_input":"2023-01-18T23:25:22.535413Z","iopub.status.idle":"2023-01-18T23:25:22.541587Z","shell.execute_reply.started":"2023-01-18T23:25:22.535366Z","shell.execute_reply":"2023-01-18T23:25:22.540104Z"},"trusted":true},"execution_count":356,"outputs":[]},{"cell_type":"markdown","source":"To get a baseline, we can start by training a single decision tree. Random forests contain many decision trees, trained on differents subsets of the dataset (often called estimators), and average their results. This theory behind this technique is that sampling from many uncorrelated estimators should produce a balaced error distribution across the many estimators. Taking the mean of the errors from each estimator should results in a mean error that is close to 0! (This assumes a balanced distribution of positive and negative errors). Thus adding more estimators, as long as they remain uncorrelated, should result in a smaller error.\n\nFirst we can load the dataset. Decision trees are a great model for training on categorical data, because they don't require data to be normalized as in a neurel net, instead they can simply split numerical data at a certain value. When we trained a linear model, we needed to use the log function to help alleviate the long tail in the 'Fare' column as well as normalize every column to be between 0 and 1. For decision trees, we simply need to make sure that our data is numeric:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ndef preprocess_df(data, is_test=False):\n    modes = data.mode().iloc[0]\n    data.fillna(modes, inplace=True)\n    data['Sex'] = pd.Categorical(data.Sex).codes\n    data['Embarked'] = pd.Categorical(data.Embarked).codes\n    inputs = data[['Fare', 'SibSp', 'Parch', 'Age', 'Sex', 'Pclass', 'Embarked']]\n    actuals = None\n    if not is_test:\n        actuals = df['Survived']\n    return inputs, actuals\n\ntrain_inputs, train_actuals = preprocess_df(df)\ntest_inputs, _ = preprocess_df(test_df, is_test=True)\ntrain_inputs.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.584616Z","iopub.execute_input":"2023-01-18T23:25:22.585485Z","iopub.status.idle":"2023-01-18T23:25:22.656455Z","shell.execute_reply.started":"2023-01-18T23:25:22.585438Z","shell.execute_reply":"2023-01-18T23:25:22.655075Z"},"trusted":true},"execution_count":357,"outputs":[{"execution_count":357,"output_type":"execute_result","data":{"text/plain":"      Fare  SibSp  Parch   Age  Sex  Pclass  Embarked\n0   7.2500      1      0  22.0    1       3         2\n1  71.2833      1      0  38.0    0       1         0\n2   7.9250      0      0  26.0    0       3         2\n3  53.1000      1      0  35.0    0       1         2\n4   8.0500      0      0  35.0    1       3         2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fare</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>Pclass</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.2500</td>\n      <td>1</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>0</td>\n      <td>38.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.9250</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53.1000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8.0500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Next we can split our training data into training and validation sets.","metadata":{}},{"cell_type":"code","source":"trn_X, val_X, trn_y, val_y = train_test_split(train_inputs, train_actuals, test_size=0.2, random_state=99)\nprint(trn_X.shape[0], val_X.shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.658652Z","iopub.execute_input":"2023-01-18T23:25:22.659286Z","iopub.status.idle":"2023-01-18T23:25:22.667692Z","shell.execute_reply.started":"2023-01-18T23:25:22.659251Z","shell.execute_reply":"2023-01-18T23:25:22.666281Z"},"trusted":true},"execution_count":358,"outputs":[{"name":"stdout","text":"712 179\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It's worth noting that our dataset is very small, so a slightly lower/higher accuracy score may not be a reliable indicator while tuning models.\n\nNext, we can train a Decision Tree Classifier. We can constrain the size of the tree by setting max_leaf_nodes to a value.","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_X, trn_y)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.669222Z","iopub.execute_input":"2023-01-18T23:25:22.669600Z","iopub.status.idle":"2023-01-18T23:25:22.682510Z","shell.execute_reply.started":"2023-01-18T23:25:22.669552Z","shell.execute_reply":"2023-01-18T23:25:22.681132Z"},"trusted":true},"execution_count":359,"outputs":[]},{"cell_type":"code","source":"import graphviz\nimport re\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\ndraw_tree(dt, trn_X, size=10)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.685525Z","iopub.execute_input":"2023-01-18T23:25:22.686313Z","iopub.status.idle":"2023-01-18T23:25:22.748207Z","shell.execute_reply.started":"2023-01-18T23:25:22.686261Z","shell.execute_reply":"2023-01-18T23:25:22.746535Z"},"trusted":true},"execution_count":360,"outputs":[{"execution_count":360,"output_type":"execute_result","data":{"text/plain":"<graphviz.files.Source at 0x7fc3b054a810>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 5.0.0 (20220707.2338)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"600pt\" height=\"363pt\"\n viewBox=\"0.00 0.00 599.50 362.60\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 358.6)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-358.6 595.5,-358.6 595.5,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#f5d1b6\" stroke=\"black\" d=\"M353,-342C353,-342 228,-342 228,-342 222,-342 216,-336 216,-330 216,-330 216,-286 216,-286 216,-280 222,-274 228,-274 228,-274 353,-274 353,-274 359,-274 365,-280 365,-286 365,-286 365,-330 365,-330 365,-336 359,-342 353,-342\"/>\n<text text-anchor=\"start\" x=\"255.5\" y=\"-326.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Sex ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"251\" y=\"-311.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.47</text>\n<text text-anchor=\"start\" x=\"237.5\" y=\"-296.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 712</text>\n<text text-anchor=\"start\" x=\"224\" y=\"-281.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [436, 276]</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#75baed\" stroke=\"black\" d=\"M269.5,-201C269.5,-201 153.5,-201 153.5,-201 147.5,-201 141.5,-195 141.5,-189 141.5,-189 141.5,-145 141.5,-145 141.5,-139 147.5,-133 153.5,-133 153.5,-133 269.5,-133 269.5,-133 275.5,-133 281.5,-139 281.5,-145 281.5,-145 281.5,-189 281.5,-189 281.5,-195 275.5,-201 269.5,-201\"/>\n<text text-anchor=\"start\" x=\"168\" y=\"-185.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Pclass ≤ 2.5</text>\n<text text-anchor=\"start\" x=\"172\" y=\"-170.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.36</text>\n<text text-anchor=\"start\" x=\"158.5\" y=\"-155.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 251</text>\n<text text-anchor=\"start\" x=\"149.5\" y=\"-140.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [58, 193]</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M271.58,-273.71C260.68,-254.54 246.89,-230.27 235.27,-209.82\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"238.25,-207.99 230.27,-201.02 232.17,-211.45 238.25,-207.99\"/>\n<text text-anchor=\"middle\" x=\"223.63\" y=\"-221.43\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node5\" class=\"node\">\n<title>2</title>\n<path fill=\"#eb9d64\" stroke=\"black\" d=\"M427.5,-201C427.5,-201 311.5,-201 311.5,-201 305.5,-201 299.5,-195 299.5,-189 299.5,-189 299.5,-145 299.5,-145 299.5,-139 305.5,-133 311.5,-133 311.5,-133 427.5,-133 427.5,-133 433.5,-133 439.5,-139 439.5,-145 439.5,-145 439.5,-189 439.5,-189 439.5,-195 433.5,-201 427.5,-201\"/>\n<text text-anchor=\"start\" x=\"326\" y=\"-185.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Pclass ≤ 1.5</text>\n<text text-anchor=\"start\" x=\"334.5\" y=\"-170.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.3</text>\n<text text-anchor=\"start\" x=\"316.5\" y=\"-155.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 461</text>\n<text text-anchor=\"start\" x=\"307.5\" y=\"-140.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [378, 83]</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge4\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M309.42,-273.71C320.32,-254.54 334.11,-230.27 345.73,-209.82\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"348.83,-211.45 350.73,-201.02 342.75,-207.99 348.83,-211.45\"/>\n<text text-anchor=\"middle\" x=\"357.37\" y=\"-221.43\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node3\" class=\"node\">\n<title>3</title>\n<path fill=\"#44a2e6\" stroke=\"black\" d=\"M119,-62.5C119,-62.5 12,-62.5 12,-62.5 6,-62.5 0,-56.5 0,-50.5 0,-50.5 0,-21.5 0,-21.5 0,-15.5 6,-9.5 12,-9.5 12,-9.5 119,-9.5 119,-9.5 125,-9.5 131,-15.5 131,-21.5 131,-21.5 131,-50.5 131,-50.5 131,-56.5 125,-62.5 119,-62.5\"/>\n<text text-anchor=\"start\" x=\"30.5\" y=\"-47.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.1</text>\n<text text-anchor=\"start\" x=\"12.5\" y=\"-32.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 136</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-17.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [7, 129]</text>\n</g>\n<!-- 1&#45;&gt;3 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M173.9,-132.78C151.93,-113.37 124.31,-88.96 102.45,-69.65\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"104.49,-66.78 94.68,-62.78 99.85,-72.02 104.49,-66.78\"/>\n</g>\n<!-- 4 -->\n<g id=\"node4\" class=\"node\">\n<title>4</title>\n<path fill=\"#d7ebfa\" stroke=\"black\" d=\"M268,-62.5C268,-62.5 161,-62.5 161,-62.5 155,-62.5 149,-56.5 149,-50.5 149,-50.5 149,-21.5 149,-21.5 149,-15.5 155,-9.5 161,-9.5 161,-9.5 268,-9.5 268,-9.5 274,-9.5 280,-15.5 280,-21.5 280,-21.5 280,-50.5 280,-50.5 280,-56.5 274,-62.5 268,-62.5\"/>\n<text text-anchor=\"start\" x=\"175\" y=\"-47.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.49</text>\n<text text-anchor=\"start\" x=\"161.5\" y=\"-32.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 115</text>\n<text text-anchor=\"start\" x=\"157\" y=\"-17.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [51, 64]</text>\n</g>\n<!-- 1&#45;&gt;4 -->\n<g id=\"edge3\" class=\"edge\">\n<title>1&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M212.27,-132.78C212.7,-114.47 213.23,-91.73 213.66,-73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"217.17,-72.86 213.9,-62.78 210.17,-72.7 217.17,-72.86\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#f5cdb1\" stroke=\"black\" d=\"M421,-62.5C421,-62.5 314,-62.5 314,-62.5 308,-62.5 302,-56.5 302,-50.5 302,-50.5 302,-21.5 302,-21.5 302,-15.5 308,-9.5 314,-9.5 314,-9.5 421,-9.5 421,-9.5 427,-9.5 433,-15.5 433,-21.5 433,-21.5 433,-50.5 433,-50.5 433,-56.5 427,-62.5 421,-62.5\"/>\n<text text-anchor=\"start\" x=\"328\" y=\"-47.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.47</text>\n<text text-anchor=\"start\" x=\"319\" y=\"-32.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 98</text>\n<text text-anchor=\"start\" x=\"310\" y=\"-17.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [61, 37]</text>\n</g>\n<!-- 2&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>2&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M368.98,-132.78C368.7,-114.47 368.35,-91.73 368.06,-73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"371.55,-72.73 367.9,-62.78 364.56,-72.83 371.55,-72.73\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#e99356\" stroke=\"black\" d=\"M579.5,-62.5C579.5,-62.5 463.5,-62.5 463.5,-62.5 457.5,-62.5 451.5,-56.5 451.5,-50.5 451.5,-50.5 451.5,-21.5 451.5,-21.5 451.5,-15.5 457.5,-9.5 463.5,-9.5 463.5,-9.5 579.5,-9.5 579.5,-9.5 585.5,-9.5 591.5,-15.5 591.5,-21.5 591.5,-21.5 591.5,-50.5 591.5,-50.5 591.5,-56.5 585.5,-62.5 579.5,-62.5\"/>\n<text text-anchor=\"start\" x=\"482\" y=\"-47.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.22</text>\n<text text-anchor=\"start\" x=\"468.5\" y=\"-32.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 363</text>\n<text text-anchor=\"start\" x=\"459.5\" y=\"-17.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [317, 46]</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M408.64,-132.78C431.62,-113.28 460.53,-88.75 483.33,-69.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"485.76,-71.92 491.12,-62.78 481.23,-66.58 485.76,-71.92\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"We can view how the decision tree fits the data by exporting the tree to an image via GraphViz. We can see that the tree first splits on *Sex*, and for good reason, as 193/251 females (sex == 0) survived, making it a great predictor. The decision tree splits females next by class, with the upper 2 classes boasting a survival rate of 129/136, while the lower class was split down the middle 64/115. On the male side, the decision tree found that the next best way to predict survival was also pclass, with first class male passengers faring much better (37/98) than second and third class males (46/363).\n\nNext, we can measure the accuracy of this model on the validation set.","metadata":{}},{"cell_type":"code","source":"mean_absolute_error(val_y, dt.predict(val_X))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.751257Z","iopub.execute_input":"2023-01-18T23:25:22.752115Z","iopub.status.idle":"2023-01-18T23:25:22.766286Z","shell.execute_reply.started":"2023-01-18T23:25:22.752048Z","shell.execute_reply":"2023-01-18T23:25:22.764998Z"},"trusted":true},"execution_count":361,"outputs":[{"execution_count":361,"output_type":"execute_result","data":{"text/plain":"0.2737430167597765"},"metadata":{}}]},{"cell_type":"markdown","source":"This error rate is pretty good result for a single decision tree, let's submit it to the competion and see how we do.","metadata":{}},{"cell_type":"code","source":"def make_submission(preds, test_df, suff):\n    test_df['Survived'] = preds\n    sub_df = test_df[['PassengerId', 'Survived']]\n    fname = f'submission.csv'\n    sub_df.to_csv(fname, index=False)\n    \n# make_submission(dt.predict(test_inputs.values), test_df, 'dt_1')","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.768283Z","iopub.execute_input":"2023-01-18T23:25:22.768744Z","iopub.status.idle":"2023-01-18T23:25:22.777972Z","shell.execute_reply.started":"2023-01-18T23:25:22.768700Z","shell.execute_reply":"2023-01-18T23:25:22.776470Z"},"trusted":true},"execution_count":362,"outputs":[]},{"cell_type":"markdown","source":"We acheived a submission score of 77.0% accuracy, which is not bad for our first attempt.","metadata":{}},{"cell_type":"markdown","source":"Next, we can try emsembling many decision trees in order to build a random forest classifier. Fortunately, sklearn already has the *RandomForestClassifier* we can use that performs the ensembling. As we spoke about before, introducing many random correlated trees by sampling random subsets of rows from the dataset should improve our accuracy by reducing mean error across the collection of trees as we add more and more. Given that the dataset is so tiny however, we may not see that much in the way of performance gains (foreshadowing). \n\nWe will start by utilizing 5 trees, with a min_samples per leaf set to 20. This should give a slightly more expanded decision tree, as the min leaf size we saw on our original tree was ~100.","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(5, min_samples_leaf = 20, oob_score=True)\nrf.fit(trn_X.values, trn_y.values)\nmean_absolute_error(val_y.values, rf.predict(val_X.values))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.779636Z","iopub.execute_input":"2023-01-18T23:25:22.781064Z","iopub.status.idle":"2023-01-18T23:25:22.820897Z","shell.execute_reply.started":"2023-01-18T23:25:22.781012Z","shell.execute_reply":"2023-01-18T23:25:22.819069Z"},"trusted":true},"execution_count":363,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:564: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  UserWarning,\n","output_type":"stream"},{"execution_count":363,"output_type":"execute_result","data":{"text/plain":"0.16759776536312848"},"metadata":{}}]},{"cell_type":"markdown","source":"Our error has gone down slightly now that we are using more than one decision tree, we can try submitting again to see if we fare better.","metadata":{}},{"cell_type":"code","source":"make_submission(rf.predict(test_inputs.values), test_df, 'rf_1')","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.823096Z","iopub.execute_input":"2023-01-18T23:25:22.824025Z","iopub.status.idle":"2023-01-18T23:25:22.842325Z","shell.execute_reply.started":"2023-01-18T23:25:22.823950Z","shell.execute_reply":"2023-01-18T23:25:22.841093Z"},"trusted":true},"execution_count":364,"outputs":[]},{"cell_type":"markdown","source":"This submission received an accuracy score of 75.1%, which is actually lower than before! I'll echo again that it's important not to read too far into this accuracy score, as our dataset is so tiny. If we were training and validation on a much larger dataset, then we could trust deviations in the accuracy of our model with more confidence.\n\nThat being said, there are still techniques we can use to try to improve our model.\n\nFor starters, we know that in theory using more estimators in our random forest should drive the error rate down. Let's hold *min_samples_leaf* constant while increasing the number of estimators to 100.","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(100, min_samples_leaf = 20, oob_score=True)\nrf.fit(trn_X.values, trn_y.values)\nmean_absolute_error(val_y.values, rf.predict(val_X.values))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:22.844153Z","iopub.execute_input":"2023-01-18T23:25:22.845390Z","iopub.status.idle":"2023-01-18T23:25:23.083294Z","shell.execute_reply.started":"2023-01-18T23:25:22.845341Z","shell.execute_reply":"2023-01-18T23:25:23.081919Z"},"trusted":true},"execution_count":365,"outputs":[{"execution_count":365,"output_type":"execute_result","data":{"text/plain":"0.22905027932960895"},"metadata":{}}]},{"cell_type":"markdown","source":"Our accuracy has actually decreased, but as I've said before and as we've discussed in class, with a dataset this small YMMV. Let's plot the accuracy across random forests of size 1 to 100:","metadata":{}},{"cell_type":"code","source":"preds = np.stack([t.predict(val_X.values) for t in rf.estimators_])\nplt.plot([mean_absolute_error(preds[:i+1].mean(0), val_y) for i in range(100)])","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:23.088394Z","iopub.execute_input":"2023-01-18T23:25:23.090340Z","iopub.status.idle":"2023-01-18T23:25:23.380074Z","shell.execute_reply.started":"2023-01-18T23:25:23.090284Z","shell.execute_reply":"2023-01-18T23:25:23.379083Z"},"trusted":true},"execution_count":366,"outputs":[{"execution_count":366,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7fc3b030f490>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"We can see that error decreases sharply as we introduce the first few additional trees, but returns flatten out as we get past 20. We would expect to see a smoother decreasing curve on larger datasets. Given our results I suppose it's worth trying a number of estimators closer to 20.\n\nAnother technique we can use when analyzing a random forest is *feature importance*, which measures how vital a given feature is for constructing decision trees. We would expect the features we saw in our original decision tree (Sex & PClass) to all be relatively important:","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(dict(cols=trn_X.columns, imp=rf.feature_importances_)).plot('cols', 'imp', 'barh', legend=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:25:23.381909Z","iopub.execute_input":"2023-01-18T23:25:23.382637Z","iopub.status.idle":"2023-01-18T23:25:23.566971Z","shell.execute_reply.started":"2023-01-18T23:25:23.382591Z","shell.execute_reply":"2023-01-18T23:25:23.566082Z"},"trusted":true},"execution_count":367,"outputs":[{"execution_count":367,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:ylabel='cols'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"It looks like some of the columns (Embarked, Parch, SibSp) are deemed relatively useless compared to other columns like Age, PClass, & Fare. Let's remove those columns from our dataset to see if simplifying can help improve our accuracy.\n\nThis technique can also be a powerful feature selection tool, even if you don't intend to use a random forest as your final model. It can help cull a field of many features into only a collection of important, predictive ones.","metadata":{}},{"cell_type":"markdown","source":"First, we'll try removing the un-important columns, and then we can try decreasing the number of trees.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ndef preprocess_df(data, is_test=False):\n    modes = data.mode().iloc[0]\n    data.fillna(modes, inplace=True)\n    data['Sex'] = pd.Categorical(data.Sex).codes\n    data['Embarked'] = pd.Categorical(data.Embarked).codes\n    inputs = data[['Fare', 'Age', 'Sex', 'Pclass']]\n    actuals = None\n    if not is_test:\n        actuals = df['Survived']\n    return inputs, actuals\n\ntrain_inputs, train_actuals = preprocess_df(df)\ntest_inputs, _ = preprocess_df(test_df, is_test=True)\ntrain_inputs.head()\ntrn_X, val_X, trn_y, val_y = train_test_split(train_inputs, train_actuals, test_size=0.2, random_state=99)\nrf = RandomForestClassifier(100, min_samples_leaf = 20, oob_score=True)\nrf.fit(trn_X.values, trn_y.values)\nmean_absolute_error(val_y.values, rf.predict(val_X.values))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:28:57.129335Z","iopub.execute_input":"2023-01-18T23:28:57.129868Z","iopub.status.idle":"2023-01-18T23:28:58.258824Z","shell.execute_reply.started":"2023-01-18T23:28:57.129826Z","shell.execute_reply":"2023-01-18T23:28:58.257607Z"},"trusted":true},"execution_count":373,"outputs":[{"execution_count":373,"output_type":"execute_result","data":{"text/plain":"0.2681564245810056"},"metadata":{}}]},{"cell_type":"markdown","source":"Here our accuracy did not fare better. We can make another Kaggle submission and then try decreasing the number of trees.","metadata":{}},{"cell_type":"code","source":"make_submission(rf.predict(test_inputs.values), test_df, 'rf_2')","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:30:18.129753Z","iopub.execute_input":"2023-01-18T23:30:18.130563Z","iopub.status.idle":"2023-01-18T23:30:18.161156Z","shell.execute_reply.started":"2023-01-18T23:30:18.130522Z","shell.execute_reply":"2023-01-18T23:30:18.159680Z"},"trusted":true},"execution_count":374,"outputs":[]},{"cell_type":"markdown","source":"For this submission we saw an accuracy of 76.7% which is better than our last but not our best yet! Let's try decreasing the number of estimators from 100 to 20.","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(20, min_samples_leaf = 20, oob_score=True)\nrf.fit(trn_X.values, trn_y.values)\nmean_absolute_error(val_y.values, rf.predict(val_X.values))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:32:06.035274Z","iopub.execute_input":"2023-01-18T23:32:06.036425Z","iopub.status.idle":"2023-01-18T23:32:06.110068Z","shell.execute_reply.started":"2023-01-18T23:32:06.036343Z","shell.execute_reply":"2023-01-18T23:32:06.108402Z"},"trusted":true},"execution_count":375,"outputs":[{"execution_count":375,"output_type":"execute_result","data":{"text/plain":"0.26256983240223464"},"metadata":{}}]},{"cell_type":"code","source":"make_submission(rf.predict(test_inputs.values), test_df, 'rf_3')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this submission we saw 76.5% accuracy, once again not an improvement. One last thing we could try is increasing the number of samples per leaf to a higher number, perhaps something close to 100 given we saw decent performance with a decision tree with leaves of that size. I'll choose min_samples_leaf=90 and give it one more try:","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(20, min_samples_leaf = 90, oob_score=True)\nrf.fit(trn_X.values, trn_y.values)\nmean_absolute_error(val_y.values, rf.predict(val_X.values))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T23:36:50.514613Z","iopub.execute_input":"2023-01-18T23:36:50.515192Z","iopub.status.idle":"2023-01-18T23:36:50.574660Z","shell.execute_reply.started":"2023-01-18T23:36:50.515148Z","shell.execute_reply":"2023-01-18T23:36:50.573390Z"},"trusted":true},"execution_count":376,"outputs":[{"execution_count":376,"output_type":"execute_result","data":{"text/plain":"0.2737430167597765"},"metadata":{}}]},{"cell_type":"code","source":"make_submission(rf.predict(test_inputs.values), test_df, 'rf_4')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We actually achieved the same performance as last time with this new min leaf size (76.5%). I'll iterate again that on this particular dataset tuning techniques are mostly a wash, but given a larger dataset they have proven effectiveness. In the future we can try utilizing these techniques on a larger dataset to see how they positively influence our accuracy.","metadata":{}}]}