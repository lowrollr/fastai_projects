{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from fastai.collab import *\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7: Collaborative Filtering\n",
    "\n",
    "In lecture #7, one of the topics we discussed was *Collaborative Filtering*, the mechanism behind reccommender systems. We walked through an example of how to build a reccommender system via creating *embeddings* for users and items being rated, and applied these ideas in order to predict user ratings and recommend items to users for the *MovieLens* dataset.\n",
    "\n",
    "In this mini-project, I'll be attempting to build a similar system for another popular reccommender dataset, the Jester *Jokes* dataset. The dataset contains 100 jokes rated by 24983 users. The ratings are on a scale of -10 to 10, with 99 being the \"null\" rating. The dataset is available at http://eigentaste.berkeley.edu/dataset/."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the dataset into a dataframe. We have to create column names manually, so I'll number the jokes 1-100 and create a user_id column that matches the index, since each row corresponds to a different user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-7.82</td>\n",
       "      <td>8.79</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-8.16</td>\n",
       "      <td>-7.52</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-9.85</td>\n",
       "      <td>4.17</td>\n",
       "      <td>-8.98</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.08</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>8.88</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>7.86</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>9.08</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.00</td>\n",
       "      <td>8.35</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>8.16</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>6.21</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8.50</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.58</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.73</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.11</td>\n",
       "      <td>6.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>-6.17</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-7.09</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>-8.69</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-6.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-6.89</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-9.08</td>\n",
       "      <td>-5.05</td>\n",
       "      <td>-3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>8.59</td>\n",
       "      <td>-9.85</td>\n",
       "      <td>7.72</td>\n",
       "      <td>8.79</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>2.33</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>6.84</td>\n",
       "      <td>3.16</td>\n",
       "      <td>9.17</td>\n",
       "      <td>-6.21</td>\n",
       "      <td>-8.16</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>9.27</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-5.19</td>\n",
       "      <td>...</td>\n",
       "      <td>7.23</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-5.68</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-3.79</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-9.42</td>\n",
       "      <td>-6.89</td>\n",
       "      <td>-8.74</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-5.29</td>\n",
       "      <td>-8.93</td>\n",
       "      <td>-7.86</td>\n",
       "      <td>...</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>4.17</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>-4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>3.01</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>3.01</td>\n",
       "      <td>6.41</td>\n",
       "      <td>5.15</td>\n",
       "      <td>8.93</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.01</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>4.47</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>-2.91</td>\n",
       "      <td>4.08</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>-5.73</td>\n",
       "      <td>99.00</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-5.29</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>6.17</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-7.23</td>\n",
       "      <td>...</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-3.30</td>\n",
       "      <td>3.45</td>\n",
       "      <td>5.44</td>\n",
       "      <td>4.08</td>\n",
       "      <td>2.48</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>5.87</td>\n",
       "      <td>99.00</td>\n",
       "      <td>5.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>7.52</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>9.22</td>\n",
       "      <td>9.27</td>\n",
       "      <td>9.22</td>\n",
       "      <td>8.30</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3.50</td>\n",
       "      <td>8.16</td>\n",
       "      <td>5.97</td>\n",
       "      <td>...</td>\n",
       "      <td>8.11</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>5.58</td>\n",
       "      <td>6.84</td>\n",
       "      <td>5.53</td>\n",
       "      <td>-5.92</td>\n",
       "      <td>8.20</td>\n",
       "      <td>8.98</td>\n",
       "      <td>-8.16</td>\n",
       "      <td>6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>8.79</td>\n",
       "      <td>-5.78</td>\n",
       "      <td>6.02</td>\n",
       "      <td>3.69</td>\n",
       "      <td>7.77</td>\n",
       "      <td>-5.83</td>\n",
       "      <td>8.69</td>\n",
       "      <td>8.59</td>\n",
       "      <td>-5.92</td>\n",
       "      <td>...</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-5.49</td>\n",
       "      <td>-8.59</td>\n",
       "      <td>8.69</td>\n",
       "      <td>-8.74</td>\n",
       "      <td>-3.01</td>\n",
       "      <td>8.30</td>\n",
       "      <td>-4.81</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-5.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-4.13</td>\n",
       "      <td>4.22</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>2.91</td>\n",
       "      <td>...</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-5.92</td>\n",
       "      <td>7.28</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>3.74</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-2.86</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>99.00</td>\n",
       "      <td>-9.27</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>-7.38</td>\n",
       "      <td>99.00</td>\n",
       "      <td>8.74</td>\n",
       "      <td>-6.31</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>3.16</td>\n",
       "      <td>7.62</td>\n",
       "      <td>3.79</td>\n",
       "      <td>8.25</td>\n",
       "      <td>4.22</td>\n",
       "      <td>7.62</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.29</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.49</td>\n",
       "      <td>8.06</td>\n",
       "      <td>0.49</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.64</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>2.52</td>\n",
       "      <td>99.00</td>\n",
       "      <td>4.13</td>\n",
       "      <td>-5.19</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>1.65</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>99.00</td>\n",
       "      <td>7.62</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>-8.64</td>\n",
       "      <td>2.43</td>\n",
       "      <td>8.93</td>\n",
       "      <td>-6.60</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id      1      2      3      4     5      6     7     8      9  ...  \\\n",
       "0         1  -7.82   8.79  -9.66  -8.16 -7.52  -8.50 -9.85  4.17  -8.98  ...   \n",
       "1         2   4.08  -0.29   6.36   4.37 -2.38  -9.66 -0.73 -5.34   8.88  ...   \n",
       "2         3  99.00  99.00  99.00  99.00  9.03   9.27  9.03  9.27  99.00  ...   \n",
       "3         4  99.00   8.35  99.00  99.00  1.80   8.16 -2.82  6.21  99.00  ...   \n",
       "4         5   8.50   4.61  -4.17  -5.39  1.36   1.60  7.04  4.61  -0.44  ...   \n",
       "5         6  -6.17  -3.54   0.44  -8.50 -7.09  -4.32 -8.69 -0.87  -6.65  ...   \n",
       "6         7  99.00  99.00  99.00  99.00  8.59  -9.85  7.72  8.79  99.00  ...   \n",
       "7         8   6.84   3.16   9.17  -6.21 -8.16  -1.70  9.27  1.41  -5.19  ...   \n",
       "8         9  -3.79  -3.54  -9.42  -6.89 -8.74  -0.29 -5.29 -8.93  -7.86  ...   \n",
       "9        10   3.01   5.15   5.15   3.01  6.41   5.15  8.93  2.52   3.01  ...   \n",
       "10       11  -2.91   4.08  99.00  99.00 -5.73  99.00  2.48 -5.29  99.00  ...   \n",
       "11       12   1.31   1.80   2.57  -2.38  0.73   0.73 -0.97  5.00  -7.23  ...   \n",
       "12       13  99.00  99.00  99.00  99.00  5.87  99.00  5.58  0.53  99.00  ...   \n",
       "13       14   9.22   9.27   9.22   8.30  7.43   0.44  3.50  8.16   5.97  ...   \n",
       "14       15   8.79  -5.78   6.02   3.69  7.77  -5.83  8.69  8.59  -5.92  ...   \n",
       "15       16  -3.50   1.55   2.33  -4.13  4.22  -2.28 -2.96 -0.49   2.91  ...   \n",
       "16       17  99.00  -9.27  99.00  99.00 -7.38  99.00  8.74 -6.31  99.00  ...   \n",
       "17       18   3.16   7.62   3.79   8.25  4.22   7.62  2.43  0.97   0.53  ...   \n",
       "18       19   4.22   3.64  99.00  99.00  2.52  99.00  4.13 -5.19  99.00  ...   \n",
       "19       20  99.00   7.62  99.00  99.00 -8.64   2.43  8.93 -6.60  99.00  ...   \n",
       "\n",
       "       91     92     93     94     95     96     97     98     99    100  \n",
       "0    2.82  99.00  99.00  99.00  99.00  99.00  -5.63  99.00  99.00  99.00  \n",
       "1    2.82  -4.95  -0.29   7.86  -0.19  -2.14   3.06   0.34  -4.32   1.07  \n",
       "2   99.00  99.00  99.00   9.08  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "3   99.00  99.00  99.00   0.53  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "4    5.19   5.58   4.27   5.19   5.73   1.55   3.11   6.55   1.80   1.60  \n",
       "5   -3.54  -6.89  -0.68  -2.96  -2.18  -3.35   0.05  -9.08  -5.05  -3.45  \n",
       "6   99.00  99.00  99.00  99.00  99.00   2.33  99.00  99.00  99.00  99.00  \n",
       "7    7.23  -1.12  -0.10  -5.68  -3.16  -3.35   2.14  -0.05   1.31   0.00  \n",
       "8    4.37  -0.29   4.17  -0.29  -0.29  -0.29  -0.29  -0.29  -3.40  -4.95  \n",
       "9   99.00   4.47  99.00  99.00  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "10  99.00   6.17  99.00  99.00  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "11   1.46   1.70   0.29  -3.30   3.45   5.44   4.08   2.48   4.51   4.66  \n",
       "12  99.00  99.00   7.52  99.00  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "13   8.11  -1.02   5.58   6.84   5.53  -5.92   8.20   8.98  -8.16   6.50  \n",
       "14   2.72  -5.49  -8.59   8.69  -8.74  -3.01   8.30  -4.81  -2.38  -5.97  \n",
       "15   3.11   1.70   0.24  -5.92   7.28  -1.36   3.74   2.82  -2.86   3.45  \n",
       "16  99.00  99.00  99.00  99.00  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "17   0.83   5.68   3.69   0.19   0.29   3.59   0.49   8.06   0.49   7.62  \n",
       "18  99.00   0.05  99.00  99.00  99.00  99.00   1.65  99.00  99.00  99.00  \n",
       "19  99.00  99.00  99.00  99.00  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "\n",
       "[20 rows x 101 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_cols = [str(i) for i in range(1, 101)]\n",
    "df = pd.read_csv('./jester-data-1.csv', names=['ratings_count'] + joke_cols)\n",
    "df.drop('ratings_count', axis=1, inplace=True)\n",
    "df['user_id'] = df.index + 1\n",
    "col = df.pop('user_id')\n",
    "df.insert(0, col.name, col)\n",
    "df.head(20)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'd be good to get this into a format that a fastai dataloader can understand, formatted like (*user_id*, *movie_id*, *rating*). Fortunately, pandas has *melt*, which does exactly this. We just need to specify which columns we'ld like to pivot vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>joke_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id joke_id  rating\n",
       "0        1       1   -7.82\n",
       "1        2       1    4.08\n",
       "2        3       1   99.00\n",
       "3        4       1   99.00\n",
       "4        5       1    8.50"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn df into user_id, movie_id, rating format\n",
    "df = df.melt(id_vars='user_id', var_name='joke_id', value_vars=joke_cols, value_name='rating')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now would also be a good time to remove any rows that don't have valid ratings (i.e where rating == 99):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>joke_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id joke_id  rating\n",
       "0        1       1   -7.82\n",
       "1        2       1    4.08\n",
       "4        5       1    8.50\n",
       "5        6       1   -6.17\n",
       "7        8       1    6.84"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['rating'] != 99]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create a dataloader. Fastai has a special *CollabDataLoaders* class built for collaborative filtering, so we can just use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>joke_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19805</td>\n",
       "      <td>7</td>\n",
       "      <td>2.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5214</td>\n",
       "      <td>88</td>\n",
       "      <td>-4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18512</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21640</td>\n",
       "      <td>47</td>\n",
       "      <td>-8.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180</td>\n",
       "      <td>51</td>\n",
       "      <td>-8.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10405</td>\n",
       "      <td>12</td>\n",
       "      <td>4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6982</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15088</td>\n",
       "      <td>30</td>\n",
       "      <td>6.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18054</td>\n",
       "      <td>25</td>\n",
       "      <td>-7.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16196</td>\n",
       "      <td>78</td>\n",
       "      <td>-4.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = CollabDataLoaders.from_df(df, item_name='joke_id', bs=64)\n",
    "dls.show_batch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll first try our hand at building a collaborative learner ourself, it's important to outline the necessary compoenents:\n",
    "\n",
    "* We need embeddings for each user and each joke id\n",
    "* We need a way to access these embeddings for certain user and joke ids\n",
    "* We also need bias terms for each user and each joke\n",
    "\n",
    "Then, when we pass in a batch of (user_id, joke_id) pairs, we can use the embeddings and bias terms to predict a rating for each pair. We can then compare these predictions to the actual ratings and use the loss to update the embeddings and bias terms.\n",
    "\n",
    "Fastai implements an 'Embedding' class, but we can also write it ourselves.\n",
    "\n",
    "First, I'll create a function to generate parameters for a single set of embeddings: a matrix of size *m* by *n* where m is the number of items and n is the number of embedding factors. I'll also create a function to generate bias terms for each item. We wrap these in nn.Parameter so that we can perform gradient descent during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(num_items, num_factors):\n",
    "    return nn.Parameter(torch.randn(num_items, num_factors)), nn.Parameter(torch.randn(num_items))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create a CollabFilter module to put everything together:\n",
    "* We create embedding matrices for users and jokes, and bias terms for users and jokes\n",
    "\n",
    "* In the forward function, we access the embeddings for each (user, joke) pair in the batch\n",
    "* We calculate the dot product of the user and joke embeddings and add the bias terms to get predictions\n",
    "* We constrain the predictions via torch.sigmoid and ensure they are within our specified rating range (-10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users  = len(dls.classes['user_id'])\n",
    "n_jokes = len(dls.classes['joke_id'])\n",
    "\n",
    "class CollabFilter(Module):\n",
    "    def __init__(self, num_users, num_jokes, num_factors, rating_range = (-10.5, 10.5)) -> None:\n",
    "        self.user_embedding, self.user_bias = create_embedding(num_users, num_factors)\n",
    "        self.joke_embedding, self.joke_bias = create_embedding(num_jokes, num_factors)\n",
    "        self.rating_range = rating_range\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        user_indices, joke_indices = x[:,0], x[:,1]\n",
    "\n",
    "        user_factors = self.user_embedding[user_indices]\n",
    "        joke_factors = self.joke_embedding[joke_indices]\n",
    "        user_bias = self.user_bias[user_indices]\n",
    "        joke_bias = self.joke_bias[joke_indices]\n",
    "\n",
    "        prediction = (user_factors * joke_factors).sum(dim=1) + user_bias + joke_bias\n",
    "        \n",
    "        return torch.sigmoid(prediction) * (self.rating_range[1] - self.rating_range[0]) + self.rating_range[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass our model to a fastai Learner to train it. We'll try using 5 embedding factors with a weight decay of 0.1. *Weight Decay* is a regularization mechanims whereby we penalize large weights in order to prevent overfitting by introducing a squared weights term to the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.125642</td>\n",
       "      <td>5.219092</td>\n",
       "      <td>01:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.385003</td>\n",
       "      <td>3.449950</td>\n",
       "      <td>01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.167202</td>\n",
       "      <td>3.248522</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.948290</td>\n",
       "      <td>3.153553</td>\n",
       "      <td>01:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.592825</td>\n",
       "      <td>3.146925</td>\n",
       "      <td>01:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.216538</td>\n",
       "      <td>3.163140</td>\n",
       "      <td>01:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf = CollabFilter(n_users, n_jokes, 40)\n",
    "learn = Learner(dls, cf, loss_func=mae)\n",
    "learn.fit_one_cycle(6, 4e-3, wd=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation error of 3.16 might not seem like the greatest result, but if my interpretation of the original collaborative filtering paper [https://goldberg.berkeley.edu/pubs/eigentaste.pdf](Goldberg et al, 2000) is correct, we actually beat their un-normalized results of ~ 3.7, albeit with 23 more years of research on our side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.075839    0.5206909   2.1809263   0.04873371  2.2790222   1.6494293\n",
      " -0.14922619  2.043972    1.6697493 ]\n",
      "[-7.82  8.79 -9.66 -8.16 -7.52 -8.5  -9.85  4.17 -8.98]\n"
     ]
    }
   ],
   "source": [
    "preds = learn.model(torch.tensor([[1, i] for i in range(1, 101)])).detach().numpy()[:9]\n",
    "actuals = df[df['user_id'] == 1]['rating'].values[:9]\n",
    "print(preds)\n",
    "print(actuals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a few predictions from the model to see that it indeed is not the most accurate, but does seem to capture that this user in particular tends to view most jokes negatively. Looking at the bias value for this user we can see that a slight negative bias is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0033, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model.user_bias[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique often seen in collaborative filtering is conveting the model to a simple neural network. We can achieve this by converting the embeddings into a single linear layer by stacking them. This also allows us to use embeddings of different sizes, since we no longer need to compute the dot product of the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollabFilterNN(Module):\n",
    "    def __init__(self, num_users, num_user_factors, num_jokes, num_joke_factors, hidden_activations, rating_range = (-10.5, 10.5)) -> None:\n",
    "        self.user_embedding, _ = create_embedding(num_users, num_user_factors)\n",
    "        self.joke_embedding, _ = create_embedding(num_jokes, num_joke_factors)\n",
    "        self.rating_range = rating_range\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_user_factors + num_joke_factors, hidden_activations),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_activations, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embs = torch.cat([self.user_embedding[x[:,0]], self.joke_embedding[x[:,1]]], dim=1)\n",
    "        x = self.layers(embs)\n",
    "        \n",
    "        return torch.sigmoid(x) * (self.rating_range[1] - self.rating_range[0]) + self.rating_range[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai provides *get_emb_sz* which will return a good choice for the number of embedding factors along each axis. We can use this to choose the embeddings sizes for users and jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24984, 464), (101, 21))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((num_users, user_factors), (num_jokes, joke_factors)) = get_emb_sz(dls)\n",
    "((num_users, user_factors), (num_jokes, joke_factors))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try training this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.592479</td>\n",
       "      <td>3.620988</td>\n",
       "      <td>13:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.493495</td>\n",
       "      <td>3.457361</td>\n",
       "      <td>13:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.409755</td>\n",
       "      <td>3.355438</td>\n",
       "      <td>13:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.260500</td>\n",
       "      <td>3.277101</td>\n",
       "      <td>13:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.041634</td>\n",
       "      <td>3.200460</td>\n",
       "      <td>13:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.855580</td>\n",
       "      <td>3.200542</td>\n",
       "      <td>13:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfnn = CollabFilterNN(num_users, user_factors, num_jokes, joke_factors, 100)\n",
    "learn = Learner(dls, cfnn, loss_func=mae)\n",
    "learn.fit_one_cycle(6, 4e-3, wd=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performed slightly worse than the previous model, we can also try again with a higher weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.606181</td>\n",
       "      <td>3.616410</td>\n",
       "      <td>13:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.499767</td>\n",
       "      <td>3.548164</td>\n",
       "      <td>13:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.472067</td>\n",
       "      <td>3.449100</td>\n",
       "      <td>13:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.264365</td>\n",
       "      <td>3.307691</td>\n",
       "      <td>13:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.143815</td>\n",
       "      <td>3.241281</td>\n",
       "      <td>14:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfnn = CollabFilterNN(num_users, user_factors, num_jokes, joke_factors, 100)\n",
    "learn = Learner(dls, cfnn, loss_func=mae)\n",
    "learn.fit_one_cycle(5, 4e-3, wd=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really much better, but is was worth a shot. We could continue to try different hyperparameters if we wanted to try to achieve better performance. \n",
    "\n",
    "At this point we could use this model as a backend for a recommender system, where we can attempt to show jokes to users that they have not seen before but we think they will like. We could also find the highest scoring jokes across all users and show those to users who have not rated any yet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "714499164d6db2eee6aed234274a1e2b8b6b7663ab8cb01ef38bcdcb5f9c772e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
