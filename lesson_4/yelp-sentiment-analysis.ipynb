{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-01-27T03:12:26.477656Z","iopub.status.busy":"2023-01-27T03:12:26.477242Z","iopub.status.idle":"2023-01-27T03:12:26.484469Z","shell.execute_reply":"2023-01-27T03:12:26.483113Z","shell.execute_reply.started":"2023-01-27T03:12:26.477613Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Lesson 4: NLP Models\n","\n","In lesson 4 we walked through an example of setting up and fine-tuning a pre-trained NLP model for the *U.S. Patents* dataset. We classified phrases on a scale from 0 to 1 depending on how similar/dissimilar they were, depending on the patent's context area. \n","\n","In this mini-project, I'm going to fine-tune an NLP model for a different sort of task, namely Sentiment Analysis. I'll be using the Yelp Reviews dataset and use transfer-learning to fine-tune an existing model and will hopefully get some nice results on the test set."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:12:26.506432Z","iopub.status.busy":"2023-01-27T03:12:26.506165Z","iopub.status.idle":"2023-01-27T03:12:29.898038Z","shell.execute_reply":"2023-01-27T03:12:29.896931Z","shell.execute_reply.started":"2023-01-27T03:12:26.506407Z"},"trusted":true},"outputs":[],"source":["from fastai.vision.all import URLs, untar_data"]},{"cell_type":"markdown","metadata":{},"source":["First let's download the dataset. If I wanted to be exauhstive I could train the model on the entire dataset, but I'd rather not waste my precious kaggle GPU credits on a purely educational project. For this mini-project, a subset (~120000 rows) of the dataset should be fine -- I guess we'll see though."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:12:29.900419Z","iopub.status.busy":"2023-01-27T03:12:29.900129Z","iopub.status.idle":"2023-01-27T03:12:46.751080Z","shell.execute_reply":"2023-01-27T03:12:46.750055Z","shell.execute_reply.started":"2023-01-27T03:12:29.900391Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      <progress value='166379520' class='' max='166373201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [166379520/166373201 00:11&lt;00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["DOWNLOAD_PATH = '/kaggle/working'\n","path = untar_data(URLs.YELP_REVIEWS_POLARITY, data=DOWNLOAD_PATH)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:12:46.753532Z","iopub.status.busy":"2023-01-27T03:12:46.752705Z","iopub.status.idle":"2023-01-27T03:12:47.484481Z","shell.execute_reply":"2023-01-27T03:12:47.483411Z","shell.execute_reply.started":"2023-01-27T03:12:46.753486Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/yelp_review_polarity_csv/train.csv', nrows=120000, names=['label', 'review'])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:12:47.487769Z","iopub.status.busy":"2023-01-27T03:12:47.487415Z","iopub.status.idle":"2023-01-27T03:12:47.503893Z","shell.execute_reply":"2023-01-27T03:12:47.502866Z","shell.execute_reply.started":"2023-01-27T03:12:47.487733Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you c...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>I'm writing this review to give you a heads up before you see this Doctor. The office staff and administration are very unprofessional. I left a message with multiple people regarding my bill, and no one ever called me back. I had to hound them to get an answer about my bill. \\n\\nSecond, and most important, make sure your insurance is going to cover Dr. Goldberg's visits and blood work. He recommended to me that I get a physical, and he knew I was a student because I told him. I got the physical done. Later, I found out my health insurance doesn't pay for preventative visits. I received an...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>All the food is great here. But the best thing they have is their wings. Their wings are simply fantastic!!  The \\\"Wet Cajun\\\" are by the best &amp; most popular.  I also like the seasoned salt wings.  Wing Night is Monday &amp; Wednesday night, $0.75 whole wings!\\n\\nThe dining area is nice. Very family friendly! The bar is very nice is well.  This place is truly a Yinzer's dream!!  \\\"Pittsburgh Dad\\\" would love this place n'at!!</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label  \\\n","0      1   \n","1      2   \n","2      1   \n","3      1   \n","4      2   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    review  \n","0  Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I...  \n","1                                                                                                          Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.  \n","2  I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you c...  \n","3  I'm writing this review to give you a heads up before you see this Doctor. The office staff and administration are very unprofessional. I left a message with multiple people regarding my bill, and no one ever called me back. I had to hound them to get an answer about my bill. \\n\\nSecond, and most important, make sure your insurance is going to cover Dr. Goldberg's visits and blood work. He recommended to me that I get a physical, and he knew I was a student because I told him. I got the physical done. Later, I found out my health insurance doesn't pay for preventative visits. I received an...  \n","4                                                                                                                                                                                All the food is great here. But the best thing they have is their wings. Their wings are simply fantastic!!  The \\\"Wet Cajun\\\" are by the best & most popular.  I also like the seasoned salt wings.  Wing Night is Monday & Wednesday night, $0.75 whole wings!\\n\\nThe dining area is nice. Very family friendly! The bar is very nice is well.  This place is truly a Yinzer's dream!!  \\\"Pittsburgh Dad\\\" would love this place n'at!!  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:12:47.505733Z","iopub.status.busy":"2023-01-27T03:12:47.505384Z","iopub.status.idle":"2023-01-27T03:12:47.514171Z","shell.execute_reply":"2023-01-27T03:12:47.512945Z","shell.execute_reply.started":"2023-01-27T03:12:47.505700Z"},"trusted":true},"outputs":[],"source":["train_df['label'] -= 1\n"]},{"cell_type":"markdown","metadata":{},"source":["I'll also want to make sure that there aren't any null values in the reviews column, we can simply replace those with an empty string"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:12:47.516652Z","iopub.status.busy":"2023-01-27T03:12:47.516225Z","iopub.status.idle":"2023-01-27T03:12:47.534548Z","shell.execute_reply":"2023-01-27T03:12:47.533613Z","shell.execute_reply.started":"2023-01-27T03:12:47.516617Z"},"trusted":true},"outputs":[],"source":["train_df.fillna('', inplace=True)\n","train_df['input'] = train_df.review"]},{"cell_type":"markdown","metadata":{},"source":["Some concepts introduced in class were *tokenization* and *numeralization*. Tokenization refers to the concept of breaking our document (in this case, the review text) into bite-sized chunks, which are then *numeralized* into numerical input that an ML model can interpret. Each NLP model provides its own specification for how it breaks text into tokens, which we can access and use via the model's *tokenizer*. So, in order to proceed, we first need to know which model we'd like to use so we can go grab its tokenizer. \n","\n","For this project I've just grabbed on off-the-shelf sentiment analysis model, *finiteautomata/bertweet-base-sentiment-analysis*. It was trained on around 40,000 tweets, and will give us a great starting point to fine-tune our model for Yelp reviews."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:12:47.536269Z","iopub.status.busy":"2023-01-27T03:12:47.535928Z","iopub.status.idle":"2023-01-27T03:13:03.410054Z","shell.execute_reply":"2023-01-27T03:13:03.408915Z","shell.execute_reply.started":"2023-01-27T03:12:47.536236Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2cc61458e6a94dcbb390e105527f8b8c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/295 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a99282ff65334b80a5211647be205841","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/890 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b044e2ce1fbf4393b0fbc1aee6f5aa58","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/824k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"035fb82e89a94ae69904a7c0b5ad304a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be3cbdc4878941dfb8a4bba9d721bbd5","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b678ba63d65744fa8c14165f3f45dbf2","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","model_name = 'finiteautomata/bertweet-base-sentiment-analysis'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:13:03.412281Z","iopub.status.busy":"2023-01-27T03:13:03.411708Z","iopub.status.idle":"2023-01-27T03:13:03.491297Z","shell.execute_reply":"2023-01-27T03:13:03.490380Z","shell.execute_reply.started":"2023-01-27T03:13:03.412244Z"},"trusted":true},"outputs":[],"source":["train_df = train_df[['input', 'label']]"]},{"cell_type":"markdown","metadata":{},"source":["Once we have the tokenizer handy, we can wrap it in a function with a few of the necessary parameters we need to pass to the tokenizer. \n","\n","* We need to limit reviews to a length of 128 tokens, the model only supports this many. \n","* If a review is too long, we truncate it\n","* If a review is too short, we pad it"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:13:03.493204Z","iopub.status.busy":"2023-01-27T03:13:03.492828Z","iopub.status.idle":"2023-01-27T03:13:03.631902Z","shell.execute_reply":"2023-01-27T03:13:03.630957Z","shell.execute_reply.started":"2023-01-27T03:13:03.493170Z"},"trusted":true},"outputs":[],"source":["def tokenize_input(x):\n","    return tokenizer(x['input'], max_length=128, truncation=True, padding='max_length')"]},{"cell_type":"markdown","metadata":{},"source":["Then, we can convert our dataframe to a transformers Dataset (the library we'll use for everything NLP), and run our reviews through the tokenizer."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:13:03.636364Z","iopub.status.busy":"2023-01-27T03:13:03.636045Z","iopub.status.idle":"2023-01-27T03:15:09.004510Z","shell.execute_reply":"2023-01-27T03:15:09.003516Z","shell.execute_reply.started":"2023-01-27T03:13:03.636330Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e2317fd9eb14fe093df876ad02bc213","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/120 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import Dataset\n","train_ds = Dataset.from_pandas(train_df).map(tokenize_input, batched=True)\n"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T06:24:06.326698Z","iopub.status.busy":"2023-01-27T06:24:06.326334Z","iopub.status.idle":"2023-01-27T06:24:06.334311Z","shell.execute_reply":"2023-01-27T06:24:06.333110Z","shell.execute_reply.started":"2023-01-27T06:24:06.326665Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[0, 1292, 54559, 1065, 13892, 7, 6, 15297, 15, 166]"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["train_ds[0]['input_ids'][:10]"]},{"cell_type":"markdown","metadata":{},"source":["We can see that our review has been turned into a nice set of numbers!\n","\n","We also need to make sure to split our data between training and validation, let's do that now:"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:15:09.006758Z","iopub.status.busy":"2023-01-27T03:15:09.005970Z","iopub.status.idle":"2023-01-27T03:15:09.038711Z","shell.execute_reply":"2023-01-27T03:15:09.037874Z","shell.execute_reply.started":"2023-01-27T03:15:09.006720Z"},"trusted":true},"outputs":[],"source":["train_validation_dds = train_ds.train_test_split(0.2, seed=98)\n"]},{"cell_type":"markdown","metadata":{},"source":["Now we're ready to train our model. We'll need to define the necessary hyperparemeters, in this case I'll just use the ones we used in lecture."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:15:09.040816Z","iopub.status.busy":"2023-01-27T03:15:09.039940Z","iopub.status.idle":"2023-01-27T03:15:13.284383Z","shell.execute_reply":"2023-01-27T03:15:13.283408Z","shell.execute_reply.started":"2023-01-27T03:15:09.040776Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","args = TrainingArguments('outputs', learning_rate=8e-5, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n","    evaluation_strategy=\"epoch\", per_device_train_batch_size=128, per_device_eval_batch_size=128*2,\n","    num_train_epochs=4, weight_decay=0.01, report_to='none')"]},{"cell_type":"markdown","metadata":{},"source":["One important note is that this model was trained to give three output labels:\n","* 0 - Negative\n","* 1 - Neutral\n","* 2 - Positive\n","\n","However, our training set only defines:\n","* 0 - Negative\n","* 1 - Positive\n","\n","This is not a big deal, we just need to make sure to train the model to give two output labels instead of three. (You'd apply a softmax across these to get a classification).\n","\n","We'll also need to set *ignore_mismatched_sizes* to true so the model knows we will be replacing the final layer. "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:15:13.286410Z","iopub.status.busy":"2023-01-27T03:15:13.285653Z","iopub.status.idle":"2023-01-27T03:16:04.221209Z","shell.execute_reply":"2023-01-27T03:16:04.220218Z","shell.execute_reply.started":"2023-01-27T03:15:13.286371Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bbf369e0bd94762bc92a56390db76e8","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/515M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n","- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, we can start training:"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:16:04.223273Z","iopub.status.busy":"2023-01-27T03:16:04.222711Z","iopub.status.idle":"2023-01-27T03:16:09.109456Z","shell.execute_reply":"2023-01-27T03:16:09.108415Z","shell.execute_reply.started":"2023-01-27T03:16:04.223236Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cuda_amp half precision backend\n"]}],"source":["trainer = Trainer(model, args, train_dataset=train_validation_dds['train'], eval_dataset=train_validation_dds['test'],\n","                  tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T03:16:09.111492Z","iopub.status.busy":"2023-01-27T03:16:09.111129Z","iopub.status.idle":"2023-01-27T04:32:13.556466Z","shell.execute_reply":"2023-01-27T04:32:13.555414Z","shell.execute_reply.started":"2023-01-27T03:16:09.111457Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: input. If input are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 96000\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3000\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3000/3000 1:16:02, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.244800</td>\n","      <td>0.146267</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.105000</td>\n","      <td>0.145621</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.048300</td>\n","      <td>0.172505</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.016000</td>\n","      <td>0.210670</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to outputs/checkpoint-500\n","Configuration saved in outputs/checkpoint-500/config.json\n","Model weights saved in outputs/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n","added tokens file saved in outputs/checkpoint-500/added_tokens.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: input. If input are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 24000\n","  Batch size = 256\n","Saving model checkpoint to outputs/checkpoint-1000\n","Configuration saved in outputs/checkpoint-1000/config.json\n","Model weights saved in outputs/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\n","added tokens file saved in outputs/checkpoint-1000/added_tokens.json\n","Saving model checkpoint to outputs/checkpoint-1500\n","Configuration saved in outputs/checkpoint-1500/config.json\n","Model weights saved in outputs/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-1500/special_tokens_map.json\n","added tokens file saved in outputs/checkpoint-1500/added_tokens.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: input. If input are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 24000\n","  Batch size = 256\n","Saving model checkpoint to outputs/checkpoint-2000\n","Configuration saved in outputs/checkpoint-2000/config.json\n","Model weights saved in outputs/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-2000/special_tokens_map.json\n","added tokens file saved in outputs/checkpoint-2000/added_tokens.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: input. If input are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 24000\n","  Batch size = 256\n","Saving model checkpoint to outputs/checkpoint-2500\n","Configuration saved in outputs/checkpoint-2500/config.json\n","Model weights saved in outputs/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-2500/special_tokens_map.json\n","added tokens file saved in outputs/checkpoint-2500/added_tokens.json\n","Saving model checkpoint to outputs/checkpoint-3000\n","Configuration saved in outputs/checkpoint-3000/config.json\n","Model weights saved in outputs/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-3000/special_tokens_map.json\n","added tokens file saved in outputs/checkpoint-3000/added_tokens.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: input. If input are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 24000\n","  Batch size = 256\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=3000, training_loss=0.09652806345621745, metrics={'train_runtime': 4564.416, 'train_samples_per_second': 84.129, 'train_steps_per_second': 0.657, 'total_flos': 2.525866131456e+16, 'train_loss': 0.09652806345621745, 'epoch': 4.0})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["It looks like after 2 epochs our model began to overfit, as our training loss continued to go down while our validation loss began to climb back up.\n","\n","We can grab the checkpoint after the second epoch and continue from there:"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T05:01:17.704854Z","iopub.status.busy":"2023-01-27T05:01:17.703682Z","iopub.status.idle":"2023-01-27T05:01:23.119996Z","shell.execute_reply":"2023-01-27T05:01:23.118891Z","shell.execute_reply.started":"2023-01-27T05:01:17.704789Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file ./outputs/checkpoint-1500/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-sentiment-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file ./outputs/checkpoint-1500/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./outputs/checkpoint-1500.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]}],"source":["from transformers import RobertaForSequenceClassification\n","model = RobertaForSequenceClassification.from_pretrained('./outputs/checkpoint-1500')"]},{"cell_type":"markdown","metadata":{},"source":["Now we can run our model against the test set to see how we perform:"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T06:00:24.016644Z","iopub.status.busy":"2023-01-27T06:00:24.015718Z","iopub.status.idle":"2023-01-27T06:00:55.692361Z","shell.execute_reply":"2023-01-27T06:00:55.691409Z","shell.execute_reply.started":"2023-01-27T06:00:24.016593Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20a819ca30dd4ba793a063ba3927edec","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/38 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["test_df = pd.read_csv('/kaggle/working/yelp_review_polarity_csv/test.csv', nrows=120000, names=['label', 'review'])\n","test_df['label'] -= 1\n","test_df.fillna('', inplace=True)\n","test_df['input'] = test_df.review\n","test_df = test_df[['input', 'label']]\n","test_ds = Dataset.from_pandas(test_df).map(tokenize_input, batched=True)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T06:01:14.354378Z","iopub.status.busy":"2023-01-27T06:01:14.353718Z","iopub.status.idle":"2023-01-27T06:01:14.368409Z","shell.execute_reply":"2023-01-27T06:01:14.367530Z","shell.execute_reply.started":"2023-01-27T06:01:14.354341Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cuda_amp half precision backend\n"]}],"source":["eval_trainer = Trainer(model, args, eval_dataset=test_ds)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T06:01:20.190450Z","iopub.status.busy":"2023-01-27T06:01:20.190092Z","iopub.status.idle":"2023-01-27T06:03:33.069262Z","shell.execute_reply":"2023-01-27T06:03:33.068189Z","shell.execute_reply.started":"2023-01-27T06:01:20.190418Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: input. If input are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 38000\n","  Batch size = 256\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='149' max='149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [149/149 02:12]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.14627839624881744,\n"," 'eval_runtime': 132.8684,\n"," 'eval_samples_per_second': 285.997,\n"," 'eval_steps_per_second': 1.121}"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["eval_trainer.evaluate()"]},{"cell_type":"markdown","metadata":{},"source":["Our loss on the test set is roughly the same as our loss on our validation set, which is an encouraging sign that we didn't mess up badly."]},{"cell_type":"markdown","metadata":{},"source":["Now let's test out our tuned sentiment classifier! First I'll write a quick function to pass text through the transformer:"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T06:12:42.953949Z","iopub.status.busy":"2023-01-27T06:12:42.953568Z","iopub.status.idle":"2023-01-27T06:12:42.959723Z","shell.execute_reply":"2023-01-27T06:12:42.958803Z","shell.execute_reply.started":"2023-01-27T06:12:42.953916Z"},"trusted":true},"outputs":[],"source":["def predict_on_text(text):\n","    tokenized = tokenizer(text, return_tensors='pt').to('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    with torch.no_grad():\n","        logits = model(**tokenized).logits\n","    predicted_class = torch.argmax(logits).item()\n","    if predicted_class == 1:\n","        return 'positive'\n","    else:\n","        return 'negative'"]},{"cell_type":"markdown","metadata":{},"source":["I'd love to test it out on some real reviews for my favorite restauraunt in Chicago, *Au Cheval* (best burgers in the world)."]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T06:17:44.768649Z","iopub.status.busy":"2023-01-27T06:17:44.768298Z","iopub.status.idle":"2023-01-27T06:17:44.774640Z","shell.execute_reply":"2023-01-27T06:17:44.773271Z","shell.execute_reply.started":"2023-01-27T06:17:44.768618Z"},"trusted":true},"outputs":[],"source":["positive_review = \"Everything was delicious and all the staff were very friendly/helpful! They don’t take any reservations so we arrived around 5:30 to put our name down. We waited about an hour, a little less. \\\n","After we were seated, our server arrived quickly and took our orders. He was super attentive-he removed empty plates quickly and refilled our drinks right away. \\\n","The burger was perfect, as expected. The salad was bright and fresh, a great complement to the heavy main course. The fries were perfectly crispy and salty.\"\n","\n","neutral_ish_review = \"Not as good as it used to be but still solid. Probably not worth waiting for anymore, but definitely worth it without the line.\"\n","\n","negative_review = \"I really don’t get it. \\\n","We waited two hours after putting our name down. They text you when the table is ready and you have 10 minutes to show up before you lose your spot. \\\n","The burger was painfully average, even with the added egg and bacon. The $9 side of fries were also… just plain old fries. \\\n","I’m usually not one to be overly-critical, but seriously not worth the hype. Maybe my expectations were just much too high thanks to reading reviews. \\\n","Regardless, save your time and money by going elsewhere.\"\n","# don't listen to this, you should go"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T06:17:46.836637Z","iopub.status.busy":"2023-01-27T06:17:46.836251Z","iopub.status.idle":"2023-01-27T06:17:46.856287Z","shell.execute_reply":"2023-01-27T06:17:46.855260Z","shell.execute_reply.started":"2023-01-27T06:17:46.836594Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'positive'"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["predict_on_text(positive_review)"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T06:17:58.374757Z","iopub.status.busy":"2023-01-27T06:17:58.374391Z","iopub.status.idle":"2023-01-27T06:17:58.393822Z","shell.execute_reply":"2023-01-27T06:17:58.392895Z","shell.execute_reply.started":"2023-01-27T06:17:58.374725Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'positive'"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["predict_on_text(neutral_ish_review)"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T06:18:11.212019Z","iopub.status.busy":"2023-01-27T06:18:11.211423Z","iopub.status.idle":"2023-01-27T06:18:11.230642Z","shell.execute_reply":"2023-01-27T06:18:11.229627Z","shell.execute_reply.started":"2023-01-27T06:18:11.211983Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'negative'"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["predict_on_text(negative_review)"]},{"cell_type":"markdown","metadata":{},"source":["It's interesting that our neutral review was marked as positive, I would have guessed it would be negative. Our model performed as expected on the positive and negative reviews. "]},{"cell_type":"markdown","metadata":{},"source":["That's it for this mini-project! I was able to succesfully fine-tune an existing NLP transformer and train it to perform well on Yelp reviews. This was a nice intro to the Huggingface ecosystem, it seems like there is tons of cool work going on over there, even within sentiment analysis I had endless models to choose from on modelhub. Looking forward to revisiting NLP at some point soon!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1 (main, Dec 23 2022, 09:25:23) [Clang 14.0.0 (clang-1400.0.29.202)]"},"vscode":{"interpreter":{"hash":"5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"}}},"nbformat":4,"nbformat_minor":4}
