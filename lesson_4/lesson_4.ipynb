{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Lesson 4: Getting Started with NLP\n","\n","In this mini-project I'll attempt to tackle the 'US Patent Phrase to Phrase Matching' NLP Kaggle competition that we went over in lecutre using Huggingface transformers and a pre-trained model.\n","\n","\n","The competition challenges participants to find and measure similarity in patent documents. More specifically, the dataset contains pairs of phrases, and we are tasked with rating how similar they are on a scale of 0 to 1. \n","\n","The scores are actually divided into increments of 0.25 with distinct meanings: 0.0 signifies the two phrases are unrelated, 0.25 -- somehwat related, 0.5 -- synonyms which don't have nearly the same meaning (i.e. same function, same properties), 0.75 -- synonyms which have exactly or nearly the same meaning, and 1.0 -- very close matches, maybe off by only a difference in conjugation, plurality, or stop-words.\n","\n","Let's take a closer look at the data to see what we're working with:\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:13.565220Z","iopub.status.busy":"2023-01-13T20:41:13.564674Z","iopub.status.idle":"2023-01-13T20:41:25.609920Z","shell.execute_reply":"2023-01-13T20:41:25.608751Z","shell.execute_reply.started":"2023-01-13T20:41:13.565098Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# first we need to check if we are running on Kaggle or on another machine\n","from pathlib import Path\n","import os\n","import pandas as pd\n","is_kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n","\n","# if we are on kaggle the data is already available to us, if not we need to download it first\n","if not is_kaggle:\n","    import zipfile\n","    import kaggle\n","    import datasets\n","\n","    download_path = Path('us-patent-phrase-to-phrase-matching')\n","    if not download_path.exists():\n","        kaggle.api.competition_download_cli(str(download_path))\n","        zipfile.ZipFile(f'{download_path}.zip').extractall(download_path)\n","else:\n","    download_path = Path('../input/us-patent-phrase-to-phrase-matching')\n","    ! pip install -q datasets\n","\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:25.612666Z","iopub.status.busy":"2023-01-13T20:41:25.612240Z","iopub.status.idle":"2023-01-13T20:41:25.721441Z","shell.execute_reply":"2023-01-13T20:41:25.720520Z","shell.execute_reply.started":"2023-01-13T20:41:25.612626Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>36468</th>\n","      <td>8e1386cbefd7f245</td>\n","      <td>wood article</td>\n","      <td>wooden article</td>\n","      <td>B44</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>36469</th>\n","      <td>42d9e032d1cd3242</td>\n","      <td>wood article</td>\n","      <td>wooden box</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>36470</th>\n","      <td>208654ccb9e14fa3</td>\n","      <td>wood article</td>\n","      <td>wooden handle</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>36471</th>\n","      <td>756ec035e694722b</td>\n","      <td>wood article</td>\n","      <td>wooden material</td>\n","      <td>B44</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>36472</th>\n","      <td>8d135da0b55b8c88</td>\n","      <td>wood article</td>\n","      <td>wooden substrate</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>36473 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["                     id        anchor                  target context  score\n","0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n","1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n","2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n","3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n","4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n","...                 ...           ...                     ...     ...    ...\n","36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n","36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n","36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n","36471  756ec035e694722b  wood article         wooden material     B44   0.75\n","36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n","\n","[36473 rows x 5 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(download_path/'train.csv')\n","eval_df = pd.read_csv(download_path/'test.csv')\n","df"]},{"cell_type":"markdown","metadata":{},"source":["We can view the column descriptions on the competition's data page.\n","* *id*: unique identifier for the pair of phrases\n","* *anchor*: the first phrase in the pair\n","* *target*: the second phrase in the pair\n","* *context*: CPC classification defining the subject within which the similarity is to be scored\n","* *score*: this is our label, the similarity score between the two phrases\n","\n","Most of these are self-explanatory, our label in this case is the *score* field, which is given on the aforementioned 0.25-increment scale.\n","\n","The CPC classifcation is most foreign here, it is heirarchical classifcation system originally developed by the European Patent Office. Their are actually five distinct labels generally used by the CPC system, but our label data only gives the first two. For example, a context of *A47* entails Section A, Class 47. You can read more here: https://en.wikipedia.org/wiki/Cooperative_Patent_Classification"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:25.723492Z","iopub.status.busy":"2023-01-13T20:41:25.722658Z","iopub.status.idle":"2023-01-13T20:41:25.780598Z","shell.execute_reply":"2023-01-13T20:41:25.779546Z","shell.execute_reply.started":"2023-01-13T20:41:25.723452Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>36473</td>\n","      <td>36473</td>\n","      <td>36473</td>\n","      <td>36473</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>36473</td>\n","      <td>733</td>\n","      <td>29340</td>\n","      <td>106</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>component composite coating</td>\n","      <td>composition</td>\n","      <td>H01</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>1</td>\n","      <td>152</td>\n","      <td>24</td>\n","      <td>2186</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      id                       anchor       target context\n","count              36473                        36473        36473   36473\n","unique             36473                          733        29340     106\n","top     37d61fd2272659b1  component composite coating  composition     H01\n","freq                   1                          152           24    2186"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.describe(include='object')"]},{"cell_type":"markdown","metadata":{},"source":["Speaking more on contexts, we see that this dataset includes 106 unique CPC identifiers. \n","\n","It's also interesting to note that while there are 29340 unique target phrases, there are only 733 unique anchors. It'd be interesting to see if these fields overlap or are compelely disjoint."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:25.783793Z","iopub.status.busy":"2023-01-13T20:41:25.783419Z","iopub.status.idle":"2023-01-13T20:41:25.800058Z","shell.execute_reply":"2023-01-13T20:41:25.798877Z","shell.execute_reply.started":"2023-01-13T20:41:25.783758Z"},"trusted":true},"outputs":[{"data":{"text/plain":["29815"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["pd.unique(df[['anchor', 'target']].values.ravel('K')).size"]},{"cell_type":"markdown","metadata":{},"source":["The two sets are not (completely) disjoint! If they were, we'd expect to see 29340 + 733 = 30073 unique values amongst the anchor and target fields.\n","\n","One thing we could perhaps take advantage of here is symmetry - i.e. a label for (anchor, target) should also be the same for (target, anchor) across the same context.\n","\n","Symmetrical pairs may already exist in the dataset, but we can also generate the ones that don't ourselves in order to expand our training data. It will be interesting to compare performance on the original dataset vs. our expanded dataset to see if this actually makes any difference.\n","\n","Let's make the expanded dataset.\n","\n","First we'll create a new dataframe where all the 'target' and 'anchor' values are flipped. I don't think it's a big deal that we'll now have some identical ids, as we won't be training on those anyways."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:25.802062Z","iopub.status.busy":"2023-01-13T20:41:25.801713Z","iopub.status.idle":"2023-01-13T20:41:25.828065Z","shell.execute_reply":"2023-01-13T20:41:25.827180Z","shell.execute_reply.started":"2023-01-13T20:41:25.802027Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement of pollution</td>\n","      <td>abatement</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>act of abating</td>\n","      <td>abatement</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>active catalyst</td>\n","      <td>abatement</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>eliminating process</td>\n","      <td>abatement</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>forest region</td>\n","      <td>abatement</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>36468</th>\n","      <td>8e1386cbefd7f245</td>\n","      <td>wooden article</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>36469</th>\n","      <td>42d9e032d1cd3242</td>\n","      <td>wooden box</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>36470</th>\n","      <td>208654ccb9e14fa3</td>\n","      <td>wooden handle</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>36471</th>\n","      <td>756ec035e694722b</td>\n","      <td>wooden material</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>36472</th>\n","      <td>8d135da0b55b8c88</td>\n","      <td>wooden substrate</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>36473 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["                     id                  anchor        target context  score\n","0      37d61fd2272659b1  abatement of pollution     abatement     A47   0.50\n","1      7b9652b17b68b7a4          act of abating     abatement     A47   0.75\n","2      36d72442aefd8232         active catalyst     abatement     A47   0.25\n","3      5296b0c19e1ce60e     eliminating process     abatement     A47   0.50\n","4      54c1e3b9184cb5b6           forest region     abatement     A47   0.00\n","...                 ...                     ...           ...     ...    ...\n","36468  8e1386cbefd7f245          wooden article  wood article     B44   1.00\n","36469  42d9e032d1cd3242              wooden box  wood article     B44   0.50\n","36470  208654ccb9e14fa3           wooden handle  wood article     B44   0.50\n","36471  756ec035e694722b         wooden material  wood article     B44   0.75\n","36472  8d135da0b55b8c88        wooden substrate  wood article     B44   0.50\n","\n","[36473 rows x 5 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["symmetrical_df = df.copy(deep=True)\n","symmetrical_df.update(symmetrical_df[['anchor', 'target']].rename(columns={'anchor': 'target', 'target': 'anchor'}))\n","symmetrical_df"]},{"cell_type":"markdown","metadata":{},"source":["Then, we can concat the original dataframe with the new one, and drop any duplicate (anchor, target, context) combos."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:25.829775Z","iopub.status.busy":"2023-01-13T20:41:25.829336Z","iopub.status.idle":"2023-01-13T20:41:25.883532Z","shell.execute_reply":"2023-01-13T20:41:25.882465Z","shell.execute_reply.started":"2023-01-13T20:41:25.829740Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>36468</th>\n","      <td>8e1386cbefd7f245</td>\n","      <td>wooden article</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>36469</th>\n","      <td>42d9e032d1cd3242</td>\n","      <td>wooden box</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>36470</th>\n","      <td>208654ccb9e14fa3</td>\n","      <td>wooden handle</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>36471</th>\n","      <td>756ec035e694722b</td>\n","      <td>wooden material</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>36472</th>\n","      <td>8d135da0b55b8c88</td>\n","      <td>wooden substrate</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>72667 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["                     id            anchor                  target context  \\\n","0      37d61fd2272659b1         abatement  abatement of pollution     A47   \n","1      7b9652b17b68b7a4         abatement          act of abating     A47   \n","2      36d72442aefd8232         abatement         active catalyst     A47   \n","3      5296b0c19e1ce60e         abatement     eliminating process     A47   \n","4      54c1e3b9184cb5b6         abatement           forest region     A47   \n","...                 ...               ...                     ...     ...   \n","36468  8e1386cbefd7f245    wooden article            wood article     B44   \n","36469  42d9e032d1cd3242        wooden box            wood article     B44   \n","36470  208654ccb9e14fa3     wooden handle            wood article     B44   \n","36471  756ec035e694722b   wooden material            wood article     B44   \n","36472  8d135da0b55b8c88  wooden substrate            wood article     B44   \n","\n","       score  \n","0       0.50  \n","1       0.75  \n","2       0.25  \n","3       0.50  \n","4       0.00  \n","...      ...  \n","36468   1.00  \n","36469   0.50  \n","36470   0.50  \n","36471   0.75  \n","36472   0.50  \n","\n","[72667 rows x 5 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["symmetrical_df = pd.concat([df.copy(deep=True), symmetrical_df])\n","symmetrical_df = symmetrical_df.drop_duplicates(subset=['anchor', 'target', 'context'])\n","symmetrical_df"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:25.885488Z","iopub.status.busy":"2023-01-13T20:41:25.885033Z","iopub.status.idle":"2023-01-13T20:41:25.953734Z","shell.execute_reply":"2023-01-13T20:41:25.952661Z","shell.execute_reply.started":"2023-01-13T20:41:25.885452Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>72667</td>\n","      <td>72667</td>\n","      <td>72667</td>\n","      <td>72667</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>36473</td>\n","      <td>29815</td>\n","      <td>29815</td>\n","      <td>106</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>component composite coating</td>\n","      <td>component composite coating</td>\n","      <td>H01</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>2</td>\n","      <td>152</td>\n","      <td>152</td>\n","      <td>4372</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      id                       anchor  \\\n","count              72667                        72667   \n","unique             36473                        29815   \n","top     37d61fd2272659b1  component composite coating   \n","freq                   2                          152   \n","\n","                             target context  \n","count                         72667   72667  \n","unique                        29815     106  \n","top     component composite coating     H01  \n","freq                            152    4372  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["symmetrical_df.describe(include='object')"]},{"cell_type":"markdown","metadata":{},"source":["I'm legitimately not sure if this will make a difference or perhaps even make our performance worse, so we'll train a model with the original dataset as well as with this augmented dataset."]},{"cell_type":"markdown","metadata":{},"source":["As we discussed in lecutre, we can turn this into a classifcation problem by combining our fields into a single *document*, and then use a transformers to pick the correct label from the 5 possiblities. Let's choose a different pre-trained transformer and format our input data a little differently than we did in lecture to mix things up.\n","\n","As far as I'm aware the performance of different input representations is fairly arbitrary, so we'll choose something very simple:"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:25.955603Z","iopub.status.busy":"2023-01-13T20:41:25.955148Z","iopub.status.idle":"2023-01-13T20:41:26.007039Z","shell.execute_reply":"2023-01-13T20:41:26.005980Z","shell.execute_reply.started":"2023-01-13T20:41:25.955567Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0    A47 ; abatement ; abatement of pollution\n","1            A47 ; abatement ; act of abating\n","2           A47 ; abatement ; active catalyst\n","3       A47 ; abatement ; eliminating process\n","4             A47 ; abatement ; forest region\n","Name: input, dtype: object"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# we'll add in input field to both dataframes since we'd like to compare performance\n","symmetrical_df['input'] = symmetrical_df.context + ' ; ' + symmetrical_df.anchor + ' ; ' + symmetrical_df.target\n","df['input'] = df.context + ' ; ' + df.anchor + ' ; ' + df.target\n","\n","eval_df['input'] = eval_df.context + ' ; ' + eval_df.anchor + ' ; ' + eval_df.target\n","\n","symmetrical_df.input.head()"]},{"cell_type":"markdown","metadata":{},"source":["Next we can load our pandas dataframe into a tranformer Dataset object."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:26.009016Z","iopub.status.busy":"2023-01-13T20:41:26.008637Z","iopub.status.idle":"2023-01-13T20:41:26.754191Z","shell.execute_reply":"2023-01-13T20:41:26.753260Z","shell.execute_reply.started":"2023-01-13T20:41:26.008977Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","ds = Dataset.from_pandas(df)\n","symmetrical_ds = Dataset.from_pandas(symmetrical_df)\n","eval_ds = Dataset.from_pandas(eval_df)"]},{"cell_type":"markdown","metadata":{},"source":["Next, we'll need to tokenize and numeralize our data. In order to tokenize our data we'll need to know which tokenizer our model uses -- the best pre-trained model available (just judging by descriptions) seems to be BERT for Patents (https://huggingface.co/anferico/bert-for-patents)."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:26.758707Z","iopub.status.busy":"2023-01-13T20:41:26.757816Z","iopub.status.idle":"2023-01-13T20:41:40.126937Z","shell.execute_reply":"2023-01-13T20:41:40.125904Z","shell.execute_reply.started":"2023-01-13T20:41:26.758669Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c02616985e7d4a19afddd77dc78ec1fb","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/327 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"002e65343f6247aba9cf02720a030d64","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/322k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","model_name = 'anferico/bert-for-patents'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have the correct tokenizer, we can tokenize our input data."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:40.128776Z","iopub.status.busy":"2023-01-13T20:41:40.128306Z","iopub.status.idle":"2023-01-13T20:41:45.889849Z","shell.execute_reply":"2023-01-13T20:41:45.888889Z","shell.execute_reply.started":"2023-01-13T20:41:40.128738Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a0cfa8528e04d8c9ee213791b8584bb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"339e43dae28b4c96a722e8a8035774ab","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/73 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f334541e561742e39ff5306a66beb383","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def tokenize_input(x):\n","    return tokenizer(x['input'])\n","\n","\n","tokenized_ds = ds.map(tokenize_input, batched=True)\n","tokenized_symmetrical_ds = symmetrical_ds.map(tokenize_input, batched=True)\n","tokenized_eval_ds = eval_ds.map(tokenize_input, batched=True)"]},{"cell_type":"markdown","metadata":{},"source":["Our tokenizer vocabulary has 39859 unique tokens. Finally, we'll need to ensure our label data is titled 'label' in order for it to work with the Transformer."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:45.892624Z","iopub.status.busy":"2023-01-13T20:41:45.891130Z","iopub.status.idle":"2023-01-13T20:41:45.902935Z","shell.execute_reply":"2023-01-13T20:41:45.901897Z","shell.execute_reply.started":"2023-01-13T20:41:45.892582Z"},"trusted":true},"outputs":[],"source":["tokenized_ds = tokenized_ds.rename_column('score', 'label')\n","tokenized_symmetrical_ds = tokenized_symmetrical_ds.rename_column('score', 'label')"]},{"cell_type":"markdown","metadata":{},"source":["In the lecture we split our training and test data via train_test_split, which splits the data randomly. However, we were warned against doing this in practice, so I'll use a different method to split the data. \n","\n","*This actually brings up an important point.* We've augemented our dataset, such that we have symmetry (i.e. (anchor, target) and (target, anchor) are the both in the dataset. For validation purposes, we'd like to avoid training on (A, B) and then validating on (B, A) -- that seems like a great way to artifically boost performance on our validation set while hurting performance on our evaluation set. \n","\n","Instead, we should split our data first, and then introduce symmetry into our training set, to ensure that our validation set contains (anchor, target)s not seen in our training set."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:45.905606Z","iopub.status.busy":"2023-01-13T20:41:45.904657Z","iopub.status.idle":"2023-01-13T20:41:51.161872Z","shell.execute_reply":"2023-01-13T20:41:51.160893Z","shell.execute_reply.started":"2023-01-13T20:41:45.905568Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"837a7d29143547f7918bc446e489c050","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/28 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8262055ee90741bb905e9bcbf9818918","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/55 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c71a04c8ae24d3e9605d6ccb721dae2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1746c6be3f945a08551e756d11aa10c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.model_selection import train_test_split\n","from datasets import DatasetDict\n","# first split the data\n","train_df, test_df = train_test_split(df, test_size=0.25, random_state=98)\n","\n","# then do symmetry stuff\n","symmetrical_train_df = train_df.copy(deep=True)\n","symmetrical_train_df.update(symmetrical_train_df[['anchor', 'target']].rename(columns={'anchor': 'target', 'target': 'anchor'}))\n","symmetrical_train_df = pd.concat([train_df.copy(deep=True), symmetrical_train_df])\n","symmetrical_train_df = symmetrical_train_df.drop_duplicates(subset=['anchor', 'target', 'context'])\n","\n","train_df['input'] = train_df.context + ' ; ' + train_df.anchor + ' ; ' + train_df.target\n","test_df['input'] = test_df.context + ' ; ' + test_df.anchor + ' ; ' + test_df.target\n","symmetrical_train_df['input'] = symmetrical_train_df.context + ' ; ' + symmetrical_train_df.anchor + ' ; ' + symmetrical_train_df.target\n","\n","train_ds = Dataset.from_pandas(train_df)\n","symmetrical_train_ds = Dataset.from_pandas(symmetrical_train_df)\n","test_ds = Dataset.from_pandas(test_df)\n","\n","tokenized_train_ds = train_ds.map(tokenize_input, batched=True)\n","tokenized_symmetrical_train_ds = symmetrical_train_ds.map(tokenize_input, batched=True)\n","tokenized_test_ds = test_ds.map(tokenize_input, batched=True)\n","tokenized_eval_ds = eval_ds.map(tokenize_input, batched=True)\n","\n","tokenized_train_ds = tokenized_train_ds.rename_column('score', 'label')\n","tokenized_symmetrical_train_ds = tokenized_symmetrical_train_ds.rename_column('score', 'label')\n","tokenized_test_ds = tokenized_test_ds.rename_column('score', 'label')\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:51.164219Z","iopub.status.busy":"2023-01-13T20:41:51.163556Z","iopub.status.idle":"2023-01-13T20:41:51.169607Z","shell.execute_reply":"2023-01-13T20:41:51.168629Z","shell.execute_reply.started":"2023-01-13T20:41:51.164179Z"},"trusted":true},"outputs":[],"source":["dds = DatasetDict({'train': tokenized_train_ds, 'test': tokenized_test_ds})\n","symmetrical_dds = DatasetDict({'train': tokenized_symmetrical_train_ds, 'test': tokenized_test_ds})"]},{"cell_type":"markdown","metadata":{},"source":["To summarize, we have:\n","- train_ds, our untouched training dataset\n","- symmetrical_train_ds, our augmented training dataset\n","- test_ds, our validation dataset\n","- eval_ds, our true 'test' dataset (huggingface transformers need this to be called 'test', it's confusing)\n","\n","Now we're finally ready to train the model, for now we'll use the same hyperparameters as in lecutre."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:41:51.171802Z","iopub.status.busy":"2023-01-13T20:41:51.171149Z","iopub.status.idle":"2023-01-13T20:44:09.902665Z","shell.execute_reply":"2023-01-13T20:44:09.901716Z","shell.execute_reply.started":"2023-01-13T20:41:51.171756Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f77f3bd33a24565ba725c59fda4f023","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at anferico/bert-for-patents and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import TrainingArguments,Trainer\n","import numpy as np\n","import torch \n","\n","def corr(x,y):\n","    x = np.squeeze(x)\n","    print(x,y)\n","    return np.corrcoef(x,y)[0][1]\n","def corr_d(eval_pred):\n","    return {'pearson': corr(*eval_pred)}\n","\n","bs = 64\n","epochs = 4\n","lr = 8e-6\n","\n","args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n","    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n","    num_train_epochs=epochs, weight_decay=0.01, report_to='none', save_strategy='no')\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n"]},{"cell_type":"markdown","metadata":{},"source":["First we'll try training on the original dataset"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:44:09.905520Z","iopub.status.busy":"2023-01-13T20:44:09.904718Z","iopub.status.idle":"2023-01-13T20:44:14.620630Z","shell.execute_reply":"2023-01-13T20:44:14.619582Z","shell.execute_reply.started":"2023-01-13T20:44:09.905461Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cuda_amp half precision backend\n"]}],"source":["trainer = Trainer(model, args, train_dataset=symmetrical_dds['train'], eval_dataset=symmetrical_dds['test'],\n","                  tokenizer=tokenizer, compute_metrics=corr_d)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T20:44:14.622689Z","iopub.status.busy":"2023-01-13T20:44:14.622292Z","iopub.status.idle":"2023-01-13T21:10:35.316708Z","shell.execute_reply":"2023-01-13T21:10:35.315730Z","shell.execute_reply.started":"2023-01-13T20:44:14.622651Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: context, anchor, __index_level_0__, id, input, target. If context, anchor, __index_level_0__, id, input, target are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 54494\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3408\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3408' max='3408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3408/3408 26:19, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Pearson</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.089700</td>\n","      <td>0.028949</td>\n","      <td>0.807813</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.023300</td>\n","      <td>0.020625</td>\n","      <td>0.832825</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.015200</td>\n","      <td>0.019860</td>\n","      <td>0.843177</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.013000</td>\n","      <td>0.020474</td>\n","      <td>0.843490</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: context, anchor, __index_level_0__, id, input, target. If context, anchor, __index_level_0__, id, input, target are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9119\n","  Batch size = 128\n"]},{"name":"stdout","output_type":"stream","text":["[0.166   0.02914 0.423   ... 0.1929  0.4478  0.1938 ] [0.25 0.   0.5  ... 0.5  0.25 0.5 ]\n"]},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: context, anchor, __index_level_0__, id, input, target. If context, anchor, __index_level_0__, id, input, target are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9119\n","  Batch size = 128\n"]},{"name":"stdout","output_type":"stream","text":["[0.2585  0.05643 0.5195  ... 0.2266  0.5366  0.2837 ] [0.25 0.   0.5  ... 0.5  0.25 0.5 ]\n"]},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: context, anchor, __index_level_0__, id, input, target. If context, anchor, __index_level_0__, id, input, target are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9119\n","  Batch size = 128\n"]},{"name":"stdout","output_type":"stream","text":["[0.27    0.00531 0.5073  ... 0.1583  0.56    0.2664 ] [0.25 0.   0.5  ... 0.5  0.25 0.5 ]\n"]},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: context, anchor, __index_level_0__, id, input, target. If context, anchor, __index_level_0__, id, input, target are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9119\n","  Batch size = 128\n"]},{"name":"stdout","output_type":"stream","text":["[0.298  0.0132 0.5254 ... 0.1649 0.5864 0.2815] [0.25 0.   0.5  ... 0.5  0.25 0.5 ]\n"]},{"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=3408, training_loss=0.02960195325909646, metrics={'train_runtime': 1580.6592, 'train_samples_per_second': 137.902, 'train_steps_per_second': 2.156, 'total_flos': 6704251531523964.0, 'train_loss': 0.02960195325909646, 'epoch': 4.0})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["\n","trainer.train()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Great! These are actually slightly better Pearsson scores than the model we trained in lecture.\n","\n","I mentioned I'd try running the same model with the non-augmented dataset as well... unfortunately trying to train a second model in the same notebook has caused a lot of GPU memory headaches for me. I've attempted various ways to free up GPU memory but so far all of them have ended up taking the notebook down with it! :( If I figure out a good way to do this in the future I will come back and update the notebook with those results.\n","\n","For now, let's generate our submission file."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T21:11:55.678370Z","iopub.status.busy":"2023-01-13T21:11:55.677988Z","iopub.status.idle":"2023-01-13T21:11:55.783201Z","shell.execute_reply":"2023-01-13T21:11:55.782181Z","shell.execute_reply.started":"2023-01-13T21:11:55.678338Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: context, anchor, id, input, target. If context, anchor, id, input, target are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 36\n","  Batch size = 128\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 : < :]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["array([[ 6.06933594e-01],\n","       [ 7.46093750e-01],\n","       [ 5.01464844e-01],\n","       [ 2.80029297e-01],\n","       [-1.13677979e-02],\n","       [ 4.97558594e-01],\n","       [ 2.97607422e-01],\n","       [-1.34124756e-02],\n","       [ 2.35961914e-01],\n","       [ 1.14257812e+00],\n","       [ 1.45385742e-01],\n","       [ 2.52197266e-01],\n","       [ 7.63183594e-01],\n","       [ 9.15039062e-01],\n","       [ 7.43652344e-01],\n","       [ 3.45703125e-01],\n","       [ 3.00537109e-01],\n","       [ 1.54907227e-01],\n","       [ 4.73876953e-01],\n","       [ 2.99072266e-01],\n","       [ 3.68652344e-01],\n","       [ 2.49389648e-01],\n","       [ 3.40820312e-01],\n","       [ 2.59521484e-01],\n","       [ 5.23925781e-01],\n","       [-1.23739243e-04],\n","       [ 2.77328491e-03],\n","       [ 4.13818359e-02],\n","       [-9.62066650e-03],\n","       [ 6.42578125e-01],\n","       [ 1.41723633e-01],\n","       [ 1.78833008e-02],\n","       [ 6.50390625e-01],\n","       [ 5.18066406e-01],\n","       [ 3.15917969e-01],\n","       [ 3.18359375e-01]])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["preds = trainer.predict(tokenized_eval_ds).predictions.astype(float)\n","preds"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T21:12:05.428544Z","iopub.status.busy":"2023-01-13T21:12:05.428142Z","iopub.status.idle":"2023-01-13T21:12:05.436322Z","shell.execute_reply":"2023-01-13T21:12:05.435351Z","shell.execute_reply.started":"2023-01-13T21:12:05.428490Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[0.60693359],\n","       [0.74609375],\n","       [0.50146484],\n","       [0.2800293 ],\n","       [0.        ],\n","       [0.49755859],\n","       [0.29760742],\n","       [0.        ],\n","       [0.23596191],\n","       [1.        ],\n","       [0.14538574],\n","       [0.25219727],\n","       [0.76318359],\n","       [0.91503906],\n","       [0.74365234],\n","       [0.34570312],\n","       [0.30053711],\n","       [0.15490723],\n","       [0.47387695],\n","       [0.29907227],\n","       [0.36865234],\n","       [0.24938965],\n","       [0.34082031],\n","       [0.25952148],\n","       [0.52392578],\n","       [0.        ],\n","       [0.00277328],\n","       [0.04138184],\n","       [0.        ],\n","       [0.64257812],\n","       [0.14172363],\n","       [0.0178833 ],\n","       [0.65039062],\n","       [0.51806641],\n","       [0.31591797],\n","       [0.31835938]])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["preds = np.clip(preds, 0, 1)\n","preds"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-01-13T21:15:58.122399Z","iopub.status.busy":"2023-01-13T21:15:58.122022Z","iopub.status.idle":"2023-01-13T21:15:58.178747Z","shell.execute_reply":"2023-01-13T21:15:58.177815Z","shell.execute_reply.started":"2023-01-13T21:15:58.122367Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63b22bd384ba40d5a66ace2bd2d0188b","version_major":2,"version_minor":0},"text/plain":["Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["1047"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["\n","submission = Dataset.from_dict({\n","    'id': tokenized_eval_ds['id'],\n","    'score': preds\n","})\n","\n","submission.to_csv('submission.csv', index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["I will update this notebook when I have my submission results, I was dealing with some problems around importing the pre-trained BERT model that we used without internet (for an offical submission you must run the notebook without internet). When I figure that out I will update this notebook with the official score."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"},"vscode":{"interpreter":{"hash":"714499164d6db2eee6aed234274a1e2b8b6b7663ab8cb01ef38bcdcb5f9c772e"}}},"nbformat":4,"nbformat_minor":4}
